---
layout: post
title:  "Sequence to Sequence Learning with Neural Networks"
---
# Sequence to Sequence Learning with Neural Networks

https://arxiv.org/abs/1409.3215

## 1. Introduction
Seq2Seq は入力ベクトルを受け取って異なる次元のベクトルを出力する LSTM の構造であり、
DNNs にとって困難である、可変的な出力を得ることを可能とした。
翻訳の場合、まず 1 つ目の LSTM で入力ベクトル（各単語）を逆順に受け取り、そこから得られ
る文章の意味を表現する固定次元のベクトル(the fixed- dimensional representation �)を用いて、2 つ目
の LSTM で出力する可変長のベクトル（翻訳語の単語の列）を生成する。

## 2. The Model
基本的な構造は Figure1 の通りだが、以下の 3 点が違う。

1. 前半と後半で 2 つの LSTM を使用する。これにより、計算コストを無視できるほど多くのパラ
メータも持たせられる。
2. Deep LSTM の方が Shallow LSTM よりも性能がいいので、4 層の LSTM を使用する。
3. 入力ベクトルを逆順にする。

## 3. Experiments

### 3.1 使用するデータ
WMT’14 English to French dataset を使用し、16 万単語をソース文章に、8 万単語を翻訳文章に適用
した。除外された単語は全て”<UNK>”というトークンに置き換えた。

### 3.2 デコード方法
デコードには beam search decoder を用いて、単語の部分仮説の内、確率の高い数個を残して、他
の可能性を排除するという操作を行った。

### 3.3 入力ベクトルを逆順に
こうすると精度が向上する厳密な理由はわからないが、minimal time lag 問題を解決し、誤差逆伝
播にてインプットとアウトプット間の communication の確立を容易にした。

### 3.4 詳細設定
・ 重みの初期値~� −0.08, 0.08
・ SGD を使用し、最初の 5epoch は� = 0.7でそこから 0.5epoch ごとに半減させ、7.5epochs まで学
習させた。
・ 128個のバッチを使用し、勾配を128で割った。
・ 勾配爆発問題を防ぐために、は勾配を128で割ったものを�とし、� = � 0を計算し、s > 5の
時、� = 45
6 とする。

### 3.5 実験結果
BLEU スコアにおいて、長文でも高い精度が得られた。

## 4. Related Work

## 5. Conclusion
この実験で、制限された語彙と問題仮定のない Deep LSTM が、語彙制限のない大規模の標準的な
統計的機械学習モデルを上回ったことを示した。
実験者が驚いた点は 2 つ。
・ 入力ベクトルを逆順に入れることで、精度が飛躍的に向上した点。
・ 長い文章の翻訳も先行研究と違い、逆順にしたことで高い精度が得られた点。
References
LSTM formulation
[10] A. Graves. Generating sequences with recurrent neural networks. In Arxiv preprint arXiv:1308.0850,
2013.
Minimal time lag 問題
[17] S. Hochreiter and J. Schmidhuber. LSTM can solve hard long time lag problems. 1997
