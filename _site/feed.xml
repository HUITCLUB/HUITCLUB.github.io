<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.7.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2018-02-27T16:41:42+09:00</updated><id>http://localhost:4000/</id><title type="html">北大IT</title><subtitle></subtitle><entry><title type="html">Sequence to Sequence Learning with Neural Networks</title><link href="http://localhost:4000/2018/02/27/Seq2Seq.html" rel="alternate" type="text/html" title="Sequence to Sequence Learning with Neural Networks" /><published>2018-02-27T00:00:00+09:00</published><updated>2018-02-27T00:00:00+09:00</updated><id>http://localhost:4000/2018/02/27/Seq2Seq</id><content type="html" xml:base="http://localhost:4000/2018/02/27/Seq2Seq.html">&lt;p&gt;https://arxiv.org/abs/1409.3215&lt;/p&gt;

&lt;h2 id=&quot;1-introduction&quot;&gt;1. Introduction&lt;/h2&gt;
&lt;p&gt;Seq2Seq は入力ベクトルを受け取って異なる次元のベクトルを出力する LSTM の構造であり、
DNNs にとって困難である、可変的な出力を得ることを可能とした。
翻訳の場合、まず 1 つ目の LSTM で入力ベクトル（各単語）を逆順に受け取り、そこから得られ
る文章の意味を表現する固定次元のベクトル(the fixed- dimensional representation �)を用いて、2 つ目
の LSTM で出力する可変長のベクトル（翻訳語の単語の列）を生成する。&lt;/p&gt;

&lt;h2 id=&quot;2-the-model&quot;&gt;2. The Model&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://huitclub.github.io/images/pic.jpg&quot; alt=&quot;Figure1&quot; title=&quot;Logo Title Text 1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;基本的な構造は Figure1 の通りだが、以下の 3 点が違う。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;前半と後半で 2 つの LSTM を使用する。これにより、計算コストを無視できるほど多くのパラ
メータも持たせられる。&lt;/li&gt;
  &lt;li&gt;Deep LSTM の方が Shallow LSTM よりも性能がいいので、4 層の LSTM を使用する。&lt;/li&gt;
  &lt;li&gt;入力ベクトルを逆順にする。&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;3-experiments&quot;&gt;3. Experiments&lt;/h2&gt;

&lt;h3 id=&quot;31-使用するデータ&quot;&gt;3.1 使用するデータ&lt;/h3&gt;
&lt;p&gt;WMT’14 English to French dataset を使用し、16 万単語をソース文章に、8 万単語を翻訳文章に適用
した。除外された単語は全て”&lt;UNK&gt;”というトークンに置き換えた。&lt;/UNK&gt;&lt;/p&gt;

&lt;h3 id=&quot;32-デコード方法&quot;&gt;3.2 デコード方法&lt;/h3&gt;
&lt;p&gt;デコードには beam search decoder を用いて、単語の部分仮説の内、確率の高い数個を残して、他
の可能性を排除するという操作を行った。&lt;/p&gt;

&lt;h3 id=&quot;33-入力ベクトルを逆順に&quot;&gt;3.3 入力ベクトルを逆順に&lt;/h3&gt;
&lt;p&gt;こうすると精度が向上する厳密な理由はわからないが、minimal time lag 問題を解決し、誤差逆伝
播にてインプットとアウトプット間の communication の確立を容易にした。&lt;/p&gt;

&lt;h3 id=&quot;34-詳細設定&quot;&gt;3.4 詳細設定&lt;/h3&gt;
&lt;p&gt;・ 重みの初期値~� −0.08, 0.08
・ SGD を使用し、最初の 5epoch は� = 0.7でそこから 0.5epoch ごとに半減させ、7.5epochs まで学
習させた。
・ 128個のバッチを使用し、勾配を128で割った。
・ 勾配爆発問題を防ぐために、は勾配を128で割ったものを�とし、� = � 0を計算し、s &amp;gt; 5の
時、� = 45
6 とする。&lt;/p&gt;

&lt;h3 id=&quot;35-実験結果&quot;&gt;3.5 実験結果&lt;/h3&gt;
&lt;p&gt;BLEU スコアにおいて、長文でも高い精度が得られた。&lt;/p&gt;

&lt;h2 id=&quot;4-related-work&quot;&gt;4. Related Work&lt;/h2&gt;

&lt;h2 id=&quot;5-conclusion&quot;&gt;5. Conclusion&lt;/h2&gt;
&lt;p&gt;この実験で、制限された語彙と問題仮定のない Deep LSTM が、語彙制限のない大規模の標準的な
統計的機械学習モデルを上回ったことを示した。
実験者が驚いた点は 2 つ。
・ 入力ベクトルを逆順に入れることで、精度が飛躍的に向上した点。
・ 長い文章の翻訳も先行研究と違い、逆順にしたことで高い精度が得られた点。
References
LSTM formulation
[10] A. Graves. Generating sequences with recurrent neural networks. In Arxiv preprint arXiv:1308.0850,
2013.
Minimal time lag 問題
[17] S. Hochreiter and J. Schmidhuber. LSTM can solve hard long time lag problems. 1997&lt;/p&gt;</content><author><name></name></author><summary type="html">https://arxiv.org/abs/1409.3215</summary></entry><entry><title type="html">17.4.1 일자 아치리눅스 버그 ‘Conflicting files: ca-certificates-utils: /etc/ssl/certs/ca-certificates.crt already exists in filesystem’</title><link href="http://localhost:4000/2017/04/01/archlinux-bug.html" rel="alternate" type="text/html" title="17.4.1 일자 아치리눅스 버그 'Conflicting files: ca-certificates-utils: /etc/ssl/certs/ca-certificates.crt already exists in filesystem'" /><published>2017-04-01T00:00:00+09:00</published><updated>2017-04-01T00:00:00+09:00</updated><id>http://localhost:4000/2017/04/01/archlinux-bug</id><content type="html" xml:base="http://localhost:4000/2017/04/01/archlinux-bug.html">&lt;p&gt;영어 읽을 수 있다면 그냥 여기를 보면 된다.&lt;/p&gt;

&lt;p&gt;https://www.ostechnix.com/fix-conflicting-files-ca-certificates-utils-etcsslcertsca-certificates-crt-already-exists-filesystem-error-arch-linux/&lt;/p&gt;

&lt;p&gt;한글로 보고 싶거나 필요한 것만 보고 싶은 사람들은 여기서 부터 읽으세요.&lt;/p&gt;

&lt;p&gt;현재 아치리눅스 시스템을 업그레이드 하면&lt;/p&gt;

&lt;p&gt;Conflicting files: ca-certificates-utils: /etc/ssl/certs/ca-certificates.crt already exists in filesystem&lt;/p&gt;

&lt;p&gt;라는 말이 뜨면서 업그레이드가 되지 않는다.&lt;/p&gt;

&lt;p&gt;간단한 해결 방법&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;(sudo) pacman -Syuw&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;w옵션은 download only 이다&lt;/p&gt;

&lt;p&gt;의미는 Sync-refresh-sysupgrade-download only&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;(sudo) rm /etc/ssl/certs/ca-certificates.crt&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;지운다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;(sudo) pacman -Su&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;다운 받아놓은 걸로 설치를 한다&lt;/p&gt;</content><author><name></name></author><summary type="html">영어 읽을 수 있다면 그냥 여기를 보면 된다.</summary></entry></feed>